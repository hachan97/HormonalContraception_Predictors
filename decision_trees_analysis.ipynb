{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import pyreadstat   # for reading Stata files\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "#from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib                                              # running computationally intensive tasks in parallel\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import sklearn.metrics\n",
    "\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory located in 'data' folder\n",
    "import os\n",
    "os.chdir('data')\n",
    "\n",
    "df1_raw, _ = pyreadstat.read_dta(\"rf_coh1_27.dta\")\n",
    "df2_raw, _ = pyreadstat.read_dta(\"rf_coh2_27.dta\")\n",
    "df3_raw, _ = pyreadstat.read_dta(\"rf_coh2_37.dta\")\n",
    "df4_raw, _ = pyreadstat.read_dta(\"rf_coh3_37.dta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function\n",
    "def preprocess_dataset(file_path):\n",
    "    # Read Stata datafile\n",
    "    df_raw, meta = pyreadstat.read_dta(file_path)\n",
    "    \n",
    "    # Operationalization\n",
    "    df_processed = df_raw.copy()\n",
    "    df_processed['pincnet_log'] = np.log(df_processed['pincnet'].replace(0, 1))\n",
    "    df_processed['hhincnet_log'] = np.log(df_processed['hhincnet'].replace(0, 1))\n",
    "    df_processed['pinc_decile'] = pd.qcut(df_processed['pincnet_log'], 10, labels=False, duplicates='drop') + 1\n",
    "    df_processed['hhincnet_decile'] = pd.qcut(df_processed['hhincnet_log'], 10, labels=False, duplicates='drop') + 1\n",
    "    df_processed['bmi_category'] = pd.cut(df_processed['bmi'], bins=[-np.inf, 18.5, 25, 30, np.inf], labels=[\"Underweight\", \"Normal\", \"Overweight\", \"Obese\"])\n",
    "    df_processed['relstat'] = df_processed['relstat'].map({\n",
    "        1: \"Single\", 5: \"Single\", 6: \"Single\", 9: \"Single\",\n",
    "        2: \"LAT\", 7: \"LAT\", 10: \"LAT\",\n",
    "        3: \"Cohabiting\", 8: \"Cohabiting\", 11: \"Cohabiting\",\n",
    "        4: \"Married\"\n",
    "    })\n",
    "    df_processed['nkids'] = df_processed['nkids'].apply(lambda x: x if x < 3 else 3)\n",
    "    df_processed['empl'] = df_processed['lfs'].map({\n",
    "        1: \"Education/not working\", 2: \"Education/not working\", 3: \"Education/not working\", 4: \"Education/not working\",\n",
    "        5: \"Education/not working\", 6: \"Education/not working\", 7: \"Education/not working\", 8: \"Education/not working\",\n",
    "        9: \"Full-time or self-employment\", 12: \"Full-time or self-employment\", 13: \"Full-time or self-employment\",\n",
    "        10: \"Marginal or part-time employment\", 11: \"Marginal or part-time employment\"\n",
    "    })\n",
    "    df_processed['edu'] = df_processed['isced'].map({\n",
    "        1: \"Basic\", 2: \"Basic\",\n",
    "        0: \"Intermediate\", 3: \"Intermediate\", 4: \"Intermediate\",\n",
    "        5: \"Advanced\", 6: \"Advanced\", 7: \"Advanced\", 8: \"Advanced\"\n",
    "    })\n",
    "    df_processed['migback'] = df_processed['migstatus'].map({1: 0, 2: 1, 3: 1})\n",
    "    df_processed['urban'] = df_processed['gkpol'].apply(lambda x: 0 if x in [1, 2, 3, 4] else 1)\n",
    "    df_processed['sex_often'] = df_processed['sexfreq'].apply(lambda x: 1 if x >= 4 else 0)\n",
    "    df_processed['denomination'] = df_processed['sd30'].map({\n",
    "        7: \"None\", 1: \"Roman Catholic\", 2: \"Protestant\", 3: \"Muslim\",\n",
    "        4: \"Other\", 5: \"Other\", 6: \"Other\"\n",
    "    })\n",
    "    df_processed['rel'] = df_processed['sd31'].map({\n",
    "        6: \"Never\", 5: \"Seldom\", 4: \"Occasionally\",\n",
    "        1: \"Frequent\", 2: \"Frequent\", 3: \"Frequent\"\n",
    "    })\n",
    "    df_processed['gendercomp'] = df_processed['gendercomp'].map({\n",
    "        0: \"No children\", 1: \"Equal gender composition\", \n",
    "        2: \"More boys\", 3: \"More girls\"\n",
    "    })\n",
    "    df_processed['migstatus'] = df_processed['migstatus'].map({\n",
    "        1: \"No migration background\", 2: \"1st generation\", 3: \"2nd generation\"\n",
    "    })\n",
    "    df_processed['health'] = df_processed['health'].map({\n",
    "        1: \"Bad\", 2: \"Not so good\", 3: \"Satisfactory\",\n",
    "        4: \"Good\", 5: \"Very good\"\n",
    "    })\n",
    "    df_processed['region'] = df_processed['region'].map({\n",
    "        0: \"Western Germany\", 1: \"Eastern Germany\", 2: \"Abroad\"\n",
    "    })\n",
    "    df_processed['ethni'] = df_processed['ethni'].map({\n",
    "        1: \"German native\", 2: \"Ethnic-German\", 3: \"Half-German\",\n",
    "        4: \"Turkish background\", 5: \"Other non-German background\"\n",
    "    })\n",
    "    df_processed['addchild'] = df_processed['addchild'].map({\n",
    "        0: \"No\", 1: \"Yes\", 2: \"Unsure\"\n",
    "    })\n",
    "    \n",
    "    # Convert variables to appropriate types\n",
    "    factors = ['hormon', 'hormon_iudh', 'hormon_iudnh', 'hormon_iudor', \n",
    "               'relstat', 'nkids', 'lfs', 'isced', 'migstatus', 'health', 'region', 'bula', \n",
    "               'addchild', 'gkpol', 'deadchild', 'gendercomp', 'sd30', 'sd31', \n",
    "               'abortion', 'ethni', 'empl', 'edu', 'migback', 'urban', \n",
    "               'sex_often', 'denomination', 'rel']\n",
    "    numerics = ['id', 'wave', 'age', 'pincnet', 'hhincnet', 'height', 'weight', \n",
    "                'bmi', 'val1i3', 'val1i4', 'val1i5', 'sexfreq', 'extraversion', \n",
    "                'agreeableness', 'conscientiousness', 'neuroticism', \n",
    "                'openness', 'pincnet_log', 'hhincnet_log']\n",
    "    \n",
    "    df_processed[factors] = df_processed[factors].astype('category')\n",
    "    df_processed[numerics] = df_processed[numerics].apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Select the desired variables\n",
    "    selected_columns = ['id', 'wave', 'age', 'relstat', 'nkids', \n",
    "                        'pincnet_log', 'hhincnet_log', 'pinc_decile', 'pincnet', 'hhincnet', 'hhincnet_decile',\n",
    "                        'empl', 'edu', 'ethni', 'migback', 'health', 'height', 'weight', 'bmi', 'bmi_category',\n",
    "                        'val1i3', 'val1i4', 'val1i5', 'region', 'bula', 'addchild', 'urban', 'sex_often', \n",
    "                        'deadchild', 'gendercomp', 'extraversion', 'agreeableness', 'conscientiousness', \n",
    "                        'neuroticism', 'openness', 'denomination', 'rel', 'abortion', 'hormon', 'hormon_iudh', \n",
    "                        'hormon_iudnh', 'hormon_iudor']\n",
    "    \n",
    "    df_processed = df_processed[selected_columns]\n",
    "    \n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"H:/oral_contraception_over_time/data\") # Set working directory\n",
    "\n",
    "# Run preprocessing step\n",
    "df1_processed = preprocess_dataset(\"rf_coh1_27.dta\")\n",
    "df2_processed = preprocess_dataset(\"rf_coh2_27.dta\")\n",
    "df3_processed = preprocess_dataset(\"rf_coh2_37.dta\")\n",
    "df4_processed = preprocess_dataset(\"rf_coh3_37.dta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>wave</th>\n",
       "      <th>age</th>\n",
       "      <th>relstat</th>\n",
       "      <th>nkids</th>\n",
       "      <th>pincnet_log</th>\n",
       "      <th>hhincnet_log</th>\n",
       "      <th>pinc_decile</th>\n",
       "      <th>pincnet</th>\n",
       "      <th>hhincnet</th>\n",
       "      <th>...</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>openness</th>\n",
       "      <th>denomination</th>\n",
       "      <th>rel</th>\n",
       "      <th>abortion</th>\n",
       "      <th>hormon</th>\n",
       "      <th>hormon_iudh</th>\n",
       "      <th>hormon_iudnh</th>\n",
       "      <th>hormon_iudor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1133000</td>\n",
       "      <td>11</td>\n",
       "      <td>27</td>\n",
       "      <td>Cohabiting</td>\n",
       "      <td>0</td>\n",
       "      <td>7.495542</td>\n",
       "      <td>8.294050</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.50</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>Protestant</td>\n",
       "      <td>Never</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1420000</td>\n",
       "      <td>11</td>\n",
       "      <td>27</td>\n",
       "      <td>Cohabiting</td>\n",
       "      <td>0</td>\n",
       "      <td>8.188689</td>\n",
       "      <td>8.188689</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>Protestant</td>\n",
       "      <td>Never</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1665000</td>\n",
       "      <td>13</td>\n",
       "      <td>27</td>\n",
       "      <td>Cohabiting</td>\n",
       "      <td>0</td>\n",
       "      <td>7.170120</td>\n",
       "      <td>7.170120</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>None</td>\n",
       "      <td>Never</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  wave  age     relstat nkids  pincnet_log  hhincnet_log  \\\n",
       "0  1133000    11   27  Cohabiting     0     7.495542      8.294050   \n",
       "1  1420000    11   27  Cohabiting     0     8.188689      8.188689   \n",
       "2  1665000    13   27  Cohabiting     0     7.170120      7.170120   \n",
       "\n",
       "   pinc_decile  pincnet  hhincnet  ...  conscientiousness neuroticism  \\\n",
       "0          5.0   1800.0    4000.0  ...               3.50         4.0   \n",
       "1          8.0   3600.0    3600.0  ...               4.00         2.0   \n",
       "2          3.0   1300.0    1300.0  ...               3.25         4.0   \n",
       "\n",
       "  openness denomination    rel abortion  hormon  hormon_iudh  hormon_iudnh  \\\n",
       "0      3.6   Protestant  Never      0.0     1.0          1.0           1.0   \n",
       "1      3.4   Protestant  Never      0.0     1.0          1.0           1.0   \n",
       "2      2.0         None  Never      1.0     0.0          0.0           0.0   \n",
       "\n",
       "  hormon_iudor  \n",
       "0          1.0  \n",
       "1          1.0  \n",
       "2          0.0  \n",
       "\n",
       "[3 rows x 42 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_processed.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Final DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cleaning and selection function\n",
    "def clean_select_df(df):\n",
    "    selected_columns = [\n",
    "        'wave', 'relstat', 'nkids', 'pinc_decile', 'hhincnet_decile',\n",
    "        'empl', 'edu', 'ethni', 'health', 'bmi_category',\n",
    "        'val1i3', 'val1i4', 'val1i5', 'region', \n",
    "        'addchild', 'urban', 'sex_often', \n",
    "        'extraversion', 'agreeableness', 'conscientiousness', 'neuroticism', 'openness', \n",
    "        'denomination', 'rel', 'abortion', \n",
    "        'migback', 'hormon_iudor'\n",
    "    ]\n",
    "    cleaned_df = df[selected_columns].dropna()\n",
    "    return cleaned_df\n",
    "\n",
    "# Apply the cleaning function to each processed dataset\n",
    "df1_cleaned = clean_select_df(df1_processed)\n",
    "df2_cleaned = clean_select_df(df2_processed)\n",
    "df3_cleaned = clean_select_df(df3_processed)\n",
    "df4_cleaned = clean_select_df(df4_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wave                    int64\n",
       "relstat              category\n",
       "nkids                category\n",
       "pinc_decile           float64\n",
       "hhincnet_decile       float64\n",
       "empl                 category\n",
       "edu                  category\n",
       "ethni                category\n",
       "health               category\n",
       "bmi_category         category\n",
       "val1i3                  int64\n",
       "val1i4                  int64\n",
       "val1i5                float64\n",
       "region               category\n",
       "addchild             category\n",
       "urban                category\n",
       "sex_often            category\n",
       "extraversion          float64\n",
       "agreeableness         float64\n",
       "conscientiousness     float64\n",
       "neuroticism           float64\n",
       "openness              float64\n",
       "denomination         category\n",
       "rel                  category\n",
       "abortion             category\n",
       "migback              category\n",
       "hormon_iudor         category\n",
       "dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_cleaned.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to split data\n",
    "def split_data(df, test_size=0.2, random_state=123, n_splits=7):\n",
    "\n",
    "    train_data, test_data = train_test_split(df, test_size=test_size, \n",
    "                                             stratify=df['hormon_iudor'], \n",
    "                                             random_state=random_state)\n",
    "    skf = StratifiedKFold(n_splits=n_splits, \n",
    "                          shuffle=True, \n",
    "                          random_state=random_state)\n",
    "    \n",
    "    train_indices, val_indices = next(skf.split(train_data, \n",
    "                                                train_data['hormon_iudor']))\n",
    "    train_folds = [(train_data.iloc[train_idx], \n",
    "                    train_data.iloc[val_idx]) \n",
    "                    for train_idx, val_idx in skf.split(train_data, train_data['hormon_iudor'])]\n",
    "    \n",
    "    return train_data.iloc[train_indices], train_folds, test_data\n",
    "\n",
    "# Split the data into training, validation, and test data\n",
    "df1_train, df1_folds, df1_test = split_data(df1_cleaned)\n",
    "df2_train, df2_folds, df2_test = split_data(df2_cleaned)\n",
    "df3_train, df3_folds, df3_test = split_data(df3_cleaned)\n",
    "df4_train, df4_folds, df4_test = split_data(df4_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wave</th>\n",
       "      <th>relstat</th>\n",
       "      <th>nkids</th>\n",
       "      <th>pinc_decile</th>\n",
       "      <th>hhincnet_decile</th>\n",
       "      <th>empl</th>\n",
       "      <th>edu</th>\n",
       "      <th>ethni</th>\n",
       "      <th>health</th>\n",
       "      <th>bmi_category</th>\n",
       "      <th>...</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>openness</th>\n",
       "      <th>denomination</th>\n",
       "      <th>rel</th>\n",
       "      <th>abortion</th>\n",
       "      <th>migback</th>\n",
       "      <th>hormon_iudor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>12</td>\n",
       "      <td>LAT</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Marginal or part-time employment</td>\n",
       "      <td>Advanced</td>\n",
       "      <td>Ethnic-German</td>\n",
       "      <td>Very good</td>\n",
       "      <td>Normal</td>\n",
       "      <td>...</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.75</td>\n",
       "      <td>2.2</td>\n",
       "      <td>Protestant</td>\n",
       "      <td>Never</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>11</td>\n",
       "      <td>Cohabiting</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Full-time or self-employment</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>German native</td>\n",
       "      <td>Satisfactory</td>\n",
       "      <td>Normal</td>\n",
       "      <td>...</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.4</td>\n",
       "      <td>Roman Catholic</td>\n",
       "      <td>Seldom</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>11</td>\n",
       "      <td>Cohabiting</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Education/not working</td>\n",
       "      <td>Advanced</td>\n",
       "      <td>Ethnic-German</td>\n",
       "      <td>Very good</td>\n",
       "      <td>Normal</td>\n",
       "      <td>...</td>\n",
       "      <td>4.50</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.2</td>\n",
       "      <td>Protestant</td>\n",
       "      <td>Never</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>12</td>\n",
       "      <td>Cohabiting</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Education/not working</td>\n",
       "      <td>Advanced</td>\n",
       "      <td>German native</td>\n",
       "      <td>Good</td>\n",
       "      <td>Normal</td>\n",
       "      <td>...</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.25</td>\n",
       "      <td>4.75</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2.4</td>\n",
       "      <td>None</td>\n",
       "      <td>Never</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>10</td>\n",
       "      <td>Cohabiting</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Full-time or self-employment</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>German native</td>\n",
       "      <td>Not so good</td>\n",
       "      <td>Normal</td>\n",
       "      <td>...</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.2</td>\n",
       "      <td>None</td>\n",
       "      <td>Never</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>13</td>\n",
       "      <td>LAT</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Full-time or self-employment</td>\n",
       "      <td>Advanced</td>\n",
       "      <td>Turkish background</td>\n",
       "      <td>Very good</td>\n",
       "      <td>Underweight</td>\n",
       "      <td>...</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.75</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.2</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>Never</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>11</td>\n",
       "      <td>Single</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Marginal or part-time employment</td>\n",
       "      <td>Advanced</td>\n",
       "      <td>German native</td>\n",
       "      <td>Good</td>\n",
       "      <td>Obese</td>\n",
       "      <td>...</td>\n",
       "      <td>4.75</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.25</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.2</td>\n",
       "      <td>Protestant</td>\n",
       "      <td>Occasionally</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>11</td>\n",
       "      <td>Single</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Marginal or part-time employment</td>\n",
       "      <td>Advanced</td>\n",
       "      <td>German native</td>\n",
       "      <td>Very good</td>\n",
       "      <td>Normal</td>\n",
       "      <td>...</td>\n",
       "      <td>2.25</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.6</td>\n",
       "      <td>Roman Catholic</td>\n",
       "      <td>Never</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>13</td>\n",
       "      <td>Cohabiting</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Full-time or self-employment</td>\n",
       "      <td>Advanced</td>\n",
       "      <td>German native</td>\n",
       "      <td>Not so good</td>\n",
       "      <td>Overweight</td>\n",
       "      <td>...</td>\n",
       "      <td>4.75</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3.50</td>\n",
       "      <td>4.75</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Protestant</td>\n",
       "      <td>Never</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>12</td>\n",
       "      <td>Cohabiting</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Education/not working</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>German native</td>\n",
       "      <td>Not so good</td>\n",
       "      <td>Normal</td>\n",
       "      <td>...</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.75</td>\n",
       "      <td>2.6</td>\n",
       "      <td>None</td>\n",
       "      <td>Never</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>486 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     wave     relstat nkids  pinc_decile  hhincnet_decile  \\\n",
       "61     12         LAT     0          1.0              2.0   \n",
       "601    11  Cohabiting     0          3.0              6.0   \n",
       "16     11  Cohabiting     0          1.0              7.0   \n",
       "413    12  Cohabiting     0          1.0              3.0   \n",
       "67     10  Cohabiting     0          7.0              8.0   \n",
       "..    ...         ...   ...          ...              ...   \n",
       "354    13         LAT     0          5.0             10.0   \n",
       "547    11      Single     0          5.0              3.0   \n",
       "275    11      Single     0          2.0              6.0   \n",
       "565    13  Cohabiting     0          4.0              6.0   \n",
       "393    12  Cohabiting     0          1.0              6.0   \n",
       "\n",
       "                                 empl           edu               ethni  \\\n",
       "61   Marginal or part-time employment      Advanced       Ethnic-German   \n",
       "601      Full-time or self-employment  Intermediate       German native   \n",
       "16              Education/not working      Advanced       Ethnic-German   \n",
       "413             Education/not working      Advanced       German native   \n",
       "67       Full-time or self-employment  Intermediate       German native   \n",
       "..                                ...           ...                 ...   \n",
       "354      Full-time or self-employment      Advanced  Turkish background   \n",
       "547  Marginal or part-time employment      Advanced       German native   \n",
       "275  Marginal or part-time employment      Advanced       German native   \n",
       "565      Full-time or self-employment      Advanced       German native   \n",
       "393             Education/not working  Intermediate       German native   \n",
       "\n",
       "           health bmi_category  ...  extraversion  agreeableness  \\\n",
       "61      Very good       Normal  ...          4.00           2.00   \n",
       "601  Satisfactory       Normal  ...          3.25           3.00   \n",
       "16      Very good       Normal  ...          4.50           3.75   \n",
       "413          Good       Normal  ...          2.25           2.25   \n",
       "67    Not so good       Normal  ...          3.75           4.00   \n",
       "..            ...          ...  ...           ...            ...   \n",
       "354     Very good  Underweight  ...          3.75           3.75   \n",
       "547          Good        Obese  ...          4.75           3.25   \n",
       "275     Very good       Normal  ...          2.25           3.00   \n",
       "565   Not so good   Overweight  ...          4.75           2.50   \n",
       "393   Not so good       Normal  ...          3.75           3.00   \n",
       "\n",
       "     conscientiousness neuroticism openness    denomination           rel  \\\n",
       "61                3.75        3.75      2.2      Protestant         Never   \n",
       "601               3.50        2.75      3.4  Roman Catholic        Seldom   \n",
       "16                4.00        3.00      3.2      Protestant         Never   \n",
       "413               4.75        2.75      2.4            None         Never   \n",
       "67                3.75        3.00      3.2            None         Never   \n",
       "..                 ...         ...      ...             ...           ...   \n",
       "354               3.75        2.00      3.2          Muslim         Never   \n",
       "547               4.25        3.00      2.2      Protestant  Occasionally   \n",
       "275               3.25        2.75      3.6  Roman Catholic         Never   \n",
       "565               3.50        4.75      4.2      Protestant         Never   \n",
       "393               3.25        3.75      2.6            None         Never   \n",
       "\n",
       "     abortion  migback  hormon_iudor  \n",
       "61        0.0      1.0           1.0  \n",
       "601       0.0      0.0           0.0  \n",
       "16        0.0      1.0           1.0  \n",
       "413       0.0      0.0           1.0  \n",
       "67        1.0      0.0           1.0  \n",
       "..        ...      ...           ...  \n",
       "354       0.0      1.0           1.0  \n",
       "547       0.0      0.0           0.0  \n",
       "275       0.0      0.0           1.0  \n",
       "565       0.0      0.0           0.0  \n",
       "393       0.0      0.0           1.0  \n",
       "\n",
       "[486 rows x 27 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(df_train, df_test):\n",
    "    categorical_cols = df_train.select_dtypes(include=['category']).columns\n",
    "\n",
    "    binary_cols = [col for col in categorical_cols if df_train[col].nunique() == 2]\n",
    "    binary_numeric_cols = [col for col in binary_cols if set(df_train[col].unique()) == {0, 1}]\n",
    "    non_binary_cols = list(set(categorical_cols) - set(binary_cols))\n",
    "\n",
    "    df_train_encoded = df_train.copy()\n",
    "    df_test_encoded = df_test.copy()\n",
    "\n",
    "    df_train_encoded[binary_numeric_cols] = df_train_encoded[binary_numeric_cols].astype(int)\n",
    "    df_test_encoded[binary_numeric_cols] = df_test_encoded[binary_numeric_cols].astype(int)\n",
    "\n",
    "    df_train_encoded = pd.get_dummies(df_train_encoded, columns=non_binary_cols)\n",
    "    df_test_encoded = pd.get_dummies(df_test_encoded, columns=non_binary_cols)\n",
    "\n",
    "    return df_train_encoded, df_test_encoded\n",
    "\n",
    "df1_train_encoded, df1_test_encoded = encode_data(df1_train, df1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wave</th>\n",
       "      <th>pinc_decile</th>\n",
       "      <th>hhincnet_decile</th>\n",
       "      <th>val1i3</th>\n",
       "      <th>val1i4</th>\n",
       "      <th>val1i5</th>\n",
       "      <th>urban</th>\n",
       "      <th>sex_often</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>...</th>\n",
       "      <th>addchild_No</th>\n",
       "      <th>addchild_Unsure</th>\n",
       "      <th>addchild_Yes</th>\n",
       "      <th>rel_Frequent</th>\n",
       "      <th>rel_Never</th>\n",
       "      <th>rel_Occasionally</th>\n",
       "      <th>rel_Seldom</th>\n",
       "      <th>empl_Education/not working</th>\n",
       "      <th>empl_Full-time or self-employment</th>\n",
       "      <th>empl_Marginal or part-time employment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>1</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>11</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>1</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>0</td>\n",
       "      <td>4.50</td>\n",
       "      <td>3.75</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>1</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.25</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>10</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>1</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4.00</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>13</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>0</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.75</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>11</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>0</td>\n",
       "      <td>4.75</td>\n",
       "      <td>3.25</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>11</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>13</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>0</td>\n",
       "      <td>4.75</td>\n",
       "      <td>2.50</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>0</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>486 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     wave  pinc_decile  hhincnet_decile  val1i3  val1i4  val1i5  urban  \\\n",
       "61     12          1.0              2.0       3       1     1.0  Urban   \n",
       "601    11          3.0              6.0       4       1     4.0  Rural   \n",
       "16     11          1.0              7.0       1       1     1.0  Rural   \n",
       "413    12          1.0              3.0       3       2     2.0  Urban   \n",
       "67     10          7.0              8.0       4       1     3.0  Urban   \n",
       "..    ...          ...              ...     ...     ...     ...    ...   \n",
       "354    13          5.0             10.0       2       1     4.0  Urban   \n",
       "547    11          5.0              3.0       2       1     1.0  Urban   \n",
       "275    11          2.0              6.0       2       1     4.0  Rural   \n",
       "565    13          4.0              6.0       1       1     4.0  Rural   \n",
       "393    12          1.0              6.0       2       2     2.0  Urban   \n",
       "\n",
       "     sex_often  extraversion  agreeableness  ...  addchild_No  \\\n",
       "61           1          4.00           2.00  ...        False   \n",
       "601          1          3.25           3.00  ...        False   \n",
       "16           0          4.50           3.75  ...        False   \n",
       "413          1          2.25           2.25  ...        False   \n",
       "67           1          3.75           4.00  ...        False   \n",
       "..         ...           ...            ...  ...          ...   \n",
       "354          0          3.75           3.75  ...        False   \n",
       "547          0          4.75           3.25  ...        False   \n",
       "275          0          2.25           3.00  ...        False   \n",
       "565          0          4.75           2.50  ...        False   \n",
       "393          0          3.75           3.00  ...        False   \n",
       "\n",
       "     addchild_Unsure  addchild_Yes  rel_Frequent  rel_Never  rel_Occasionally  \\\n",
       "61             False          True         False       True             False   \n",
       "601            False          True         False      False             False   \n",
       "16             False          True         False       True             False   \n",
       "413            False          True         False       True             False   \n",
       "67             False          True         False       True             False   \n",
       "..               ...           ...           ...        ...               ...   \n",
       "354            False          True         False       True             False   \n",
       "547            False          True         False      False              True   \n",
       "275            False          True         False       True             False   \n",
       "565            False          True         False       True             False   \n",
       "393            False          True         False       True             False   \n",
       "\n",
       "     rel_Seldom  empl_Education/not working  \\\n",
       "61        False                       False   \n",
       "601        True                       False   \n",
       "16        False                        True   \n",
       "413       False                        True   \n",
       "67        False                       False   \n",
       "..          ...                         ...   \n",
       "354       False                       False   \n",
       "547       False                       False   \n",
       "275       False                       False   \n",
       "565       False                       False   \n",
       "393       False                        True   \n",
       "\n",
       "     empl_Full-time or self-employment  empl_Marginal or part-time employment  \n",
       "61                               False                                   True  \n",
       "601                               True                                  False  \n",
       "16                               False                                  False  \n",
       "413                              False                                  False  \n",
       "67                                True                                  False  \n",
       "..                                 ...                                    ...  \n",
       "354                               True                                  False  \n",
       "547                              False                                   True  \n",
       "275                              False                                   True  \n",
       "565                               True                                  False  \n",
       "393                              False                                  False  \n",
       "\n",
       "[486 rows x 59 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_train_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "n_estimators_full = [10, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "max_depth_full = list(range(5,21))\n",
    "min_samples_split_full = list(range(2,41))\n",
    "\n",
    "# Initialize the model\n",
    "model_rf = RandomForestClassifier(random_state=666)\n",
    "\n",
    "# Define the distributions\n",
    "distributions = dict(n_estimators=n_estimators_full, \n",
    "                     max_depth=max_depth_full, \n",
    "                     min_samples_split=min_samples_split_full)\n",
    "\n",
    "# Initialize the RandomizedSearchCV\n",
    "clf_rf = RandomizedSearchCV(model_rf, distributions, \n",
    "                            n_iter=100, verbose=2, n_jobs=4, \n",
    "                            scoring='accuracy', refit='accuracy', \n",
    "                            cv=5, \n",
    "                            random_state=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 500 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n100 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py\", line 363, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py\", line 1301, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py\", line 1012, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\_array_api.py\", line 751, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\generic.py\", line 2153, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'Single'\n\n--------------------------------------------------------------------------------\n400 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py\", line 363, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py\", line 1301, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py\", line 1012, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\_array_api.py\", line 751, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\generic.py\", line 2153, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'LAT'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m search \u001b[38;5;241m=\u001b[39m \u001b[43mclf_rf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf1_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhormon_iudor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf1_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhormon_iudor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Print the best estimator and score\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(search\u001b[38;5;241m.\u001b[39mbest_estimator_)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:1018\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1013\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1014\u001b[0m     )\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1018\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:1959\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1957\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1958\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1959\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1961\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m   1962\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1963\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:995\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    993\u001b[0m     )\n\u001b[1;32m--> 995\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    997\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    999\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py:529\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    523\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    524\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    528\u001b[0m     )\n\u001b[1;32m--> 529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    538\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    539\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 500 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n100 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py\", line 363, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py\", line 1301, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py\", line 1012, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\_array_api.py\", line 751, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\generic.py\", line 2153, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'Single'\n\n--------------------------------------------------------------------------------\n400 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py\", line 363, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py\", line 1301, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py\", line 1012, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\_array_api.py\", line 751, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\chanho\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\generic.py\", line 2153, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'LAT'\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "search = clf_rf.fit(df1_train_encoded.drop('hormon_iudor', axis=1), df1_train_encoded['hormon_iudor'])\n",
    "\n",
    "# Print the best estimator and score\n",
    "print(search.best_estimator_)\n",
    "print(search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the final model\n",
    "rf_final = RandomForestClassifier(max_depth=search.best_estimator_.max_depth, min_samples_split=search.best_estimator_.min_samples_split, n_estimators=search.best_estimator_.n_estimators, random_state=666)\n",
    "rf_final.fit(df1_train.drop('hormon_iudor', axis=1), df1_train['hormon_iudor'])\n",
    "\n",
    "# Make predictions\n",
    "pred_test_rf = rf_final.predict(df1_test.drop('hormon_iudor', axis=1))\n",
    "\n",
    "# Print the accuracy and confusion matrix\n",
    "print(accuracy_score(df1_test['hormon_iudor'], pred_test_rf))\n",
    "print(confusion_matrix(df1_test['hormon_iudor'], pred_test_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best hyperparameters\n",
    "best_params_df1 = df1_grid_search_refined.best_params_\n",
    "best_params_df2 = df2_grid_search_refined.best_params_\n",
    "best_params_df3 = df3_grid_search_refined.best_params_\n",
    "best_params_df4 = df4_grid_search_refined.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the XGBoost model\n",
    "model_xgb = GradientBoostingClassifier(random_state=666)\n",
    "\n",
    "# Define the distributions\n",
    "distributions_xgb = dict(n_estimators=n_estimators_full, \n",
    "                         max_depth=max_depth_full, \n",
    "                         min_samples_split=min_samples_split_full)\n",
    "\n",
    "# Initialize the RandomizedSearchCV\n",
    "clf_xgb = RandomizedSearchCV(model_xgb, distributions_xgb, \n",
    "                             n_iter=100, verbose=2, n_jobs=4, \n",
    "                             scoring='accuracy', refit='accuracy', \n",
    "                             cv=5, \n",
    "                             random_state=666)\n",
    "\n",
    "# Fit the model\n",
    "search_xgb = clf_xgb.fit(df1_train_encoded.drop('hormon_iudor', axis=1), df1_train_encoded['hormon_iudor'])\n",
    "\n",
    "# Print the best estimator and score\n",
    "print(search_xgb.best_estimator_)\n",
    "\n",
    "# Train the final model\n",
    "xgb_final = GradientBoostingClassifier(max_depth=search_xgb.best_estimator_.max_depth, min_samples_split=search_xgb.best_estimator_.min_samples_split, n_estimators=search_xgb.best_estimator_.n_estimators, random_state=666)\n",
    "\n",
    "xgb_final.fit(df1_train.drop('hormon_iudor', axis=1), df1_train['hormon_iudor'])\n",
    "\n",
    "# Make predictions\n",
    "pred_test_xgb = xgb_final.predict(df1_test.drop('hormon_iudor', axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
